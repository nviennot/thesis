\chapter{Introduction}
\label{ch:intro}

Application record and replay is the ability to record application execution and
replay it at a later time, possibly on a different host. Record-replay has many
use cases including diagnosing and debugging applications by capturing and
reproducing hard to find bugs, providing fault tolerance and scaling
capabilities to applications by replaying application state on a replica.
The desired semantics of record-replay systems varies depending on the use case.
For example, when debugging a rarely occurring multi-threaded related bug,
a record-replay system that guarantees an identical reenactment of the original
execution is useful. By recording application execution in production
environment, a developer can capture a rare occurrence of a bug, and replay it
in his development environment, exactly as it happened, repeatedly, to analyze
the problem root cause. Record-replay systems that guarantee a faithful replay
are called deterministic. While deterministic replay is certainly useful,
there are cases where performing a faithful replay is undesirable, for example
when performing intrusive instrumentation. A developer may want to inject faults
into a replayed execution to observe how the application behaves upon failures, or
enable full logging capabilities of the application by changing a configuration
file. Here, a record-replay system tolerant to application changes would be useful.
Other record-replay use cases include database replication. For example,
recording a primary database execution and replaying it live on an other
replica provides fault tolerance. Upon primary failure, the replica is
promoted to primary, and traffic redirected. Deterministic replay alone can
provide fault tolerance, but cannot provide any scaling capabilities.
If the replica was able to serve read-only queries while being replayed,
it could offload some traffic from the primary. Deterministic replay does
not allow any deviation from the original execution, and thus is not suitable
for such scaling case. This dissertation shows that record-replay is useful
beyond deterministic replay. We show four record-replay systems. We start
by providing a deterministic record-replay system, and gradually relax the
replay faithfulness guarantee throughout the other systems to open the range of
use cases.

First, we present \scribe (\S\ref{ch:scribe}), a multiprocessor deterministic
record-replay system. To support a wide range of application, \scribe is
implemented at the operating system (OS) level, and is transparent to applications.
Implementing deterministic execution record-replay can be challenging for a
variety of reasons. First, all sources of nondeterminism must be captured and
recorded.  Sources of nondeterminism can be categorized in 1) data related
nondeterminism and 2) timing related nondeterminism. Data related nondeterminism
is the easiest to capture. It includes recording all external inputs such as
incoming network packets, or user keystrokes. Timing related nondeterminism is
harder to capture, especially when the recorded application runs on multiple
CPUs simultaneously.  Application threads or processes access data and resources
in an undefined order, resulting in many different possible schedules. It is
sufficient to record all sources of nondeterminism, both data and timing
sources, to replay an application execution deterministically.  However, doing
so with good performance is crucial since recording is done in deployed systems.

To incur minimal record overhead, \scribe introduces two new lightweight OS
mechanisms, rendezvous and sync points, to efficiently record nondeterministic
interactions such as related system calls, signals, and shared memory accesses.
For example, when two concurrent processes access the same file, the original
order in which each access took place must be preserved when replaying.  Instead
of recording the kernel scheduling decisions, \scribe piggy backs on existing
kernel synchronization primitives such as inode mutexes or file descriptor
locks, and records in which order these locks are taken by the kernel.  We call
these rendezvous points, and they make a partial ordering of execution based on
system call dependencies sufficient for replay, avoiding the recording overhead
of maintaining an exact execution ordering.

Another difficult problem to achieve deterministic replay is to replay
asynchronous interactions that can occur at arbitrary times during the
execution. For example, POSIX signal delivery may occur at any point in the
application instruction flow. Previous works have developed complex techniques
of instrumenting applications with the use of hardware counters to precisely
locate where a signal was delivered in the instruction flow.
\scribe takes a different approach by delaying these asynchronous events
during the recording to a point where it is much easier to replay
deterministically. For example, when an application is being recorded,
instead of delivering a signal in the middle of the instruction flow, \scribe
delivers the signal at the next encountered sync point, such as a system call or
a deterministic page fault. In other words,
Sync points allow \scribe to convert asynchronous interactions that can occur at
arbitrary times into synchronous events that are much easier to record and
replay. In our evaluation, we show that the introduced delay is imperceptible
in an application as sync points occur very frequently.

With multi-threaded applications, replaying accesses to shared memory is
difficult. The order in which each thread accesses a shared memory location must
be replayed deterministically. To solve this problem, instead of doing any sort
of binary instrumentation, \scribe leverages the MMU to monitor memory accesses
through page faults. \scribe implements a concurrent read, exclusive write
(CREW) protocol on shared pages. To do so, instead of having all threads share a
common page table, threads access memory through a per-thread page table, which
\scribe uses to record access order with rendezvous points and sync points.
At a given point in time, a page may be accessed by only a single thread, the
owner. When another thread tries to access that same page, a fault occurs,
and an ownership relinquish request is issued to the current owner, which is
processed at its next sync point. This lightweight mechanism allows low
overhead of recording shared memory interactions.

Our results show for the first time that (1) sync points are an effective,
lightweight mechanism for handling nondeterminism due to signals and shared
memory, (2) sync points occur often enough in real server and desktop
applications that the vast majority of asynchronous events are handled
instantaneously, and even when events are deferred, they are delayed for 25 to
220\us{} on average, (3) an operating system mechanism can record-replay real
multi-threaded and multi-process applications, (4) transparent, low-overhead
record-replay can be done for workloads across a wide range of server and
desktop applications, including Apache, MySQL, Firefox, Acrobat, OpenOffice,
parallel make, and MPlayer.  On a 4-CPU multiprocessor, \scribe{}'s recording
overhead was under 2.5\% for server applications, and less than 15\% for desktop
applications.  These results show for the first time a new level of transparent
record and replay performance on commodity multiprocessor systems that was not
previously possible. 

Second, we present \racepro (\S\ref{ch:racepro}), a process race detection
system to improve software correctness. Process races occur when multiple
processes access shared operating system resources, such as files, without
proper synchronization.
To better understand process races, we present the first study of real process
races. We study hundreds of real applications across six Linux distributions and
show that process races are numerous and a real threat to reliability and
security. Detecting harmful races is difficult for three key challenges.
The first is scope: process races are extremely heterogeneous.  They may involve
many different programs.  These programs may be written in different programming
languages, run within different processes or threads, and access diverse
resources. The second challenge is coverage: although
process races are numerous, each particular process race tends to be highly
elusive.  They are timing-dependent, and tend to surface only in rare
executions. Arguably worse than thread races, they may occur only under
specific software, hardware, and user configurations at specific sites.  It is
hopeless to rely on a few software vendors and beta testing sites to create all
possible configurations and executions for checking.  The third challenge is
algorithmic: what race detection algorithm can be used for detecting process races?
Existing algorithms assume well-defined load and store instructions and thread
synchronization primitives. However, the effects of system calls are often
under-specified and process synchronization primitives are very different from
those used in shared memory.

\racepro addresses these challenges with four ideas.  First, it checks deployed
systems in vivo.  While a deployed system is running, \racepro records
the execution without doing any checking.  \racepro then systematically checks
this recorded execution for races \emph{offline}.  By checking deployed systems,
\racepro mitigates the coverage challenge because all user machines together can
create a much larger and useful set of configurations and executions for
checking.  By decoupling recording and checking, \racepro reduces its
performance overhead on the deployed systems.
Second, \racepro uses the application transparent \scribe engine to record
deployed applications, mitigating the scope challenge, as no application source
code or modifications of the checked applications are required.
Third, to detect process races in a recorded execution,
\racepro models each system call by what we call \emph{load and store
micro-operations} to shared kernel objects.  \racepro leverages \scribe's
rendezvous points to facilitate the modeling of these two operations with low
overhead.  Because these two operations are well understood by existing race
detection algorithms, \racepro can leverage these algorithms, mitigating the
algorithmic challenge.  Fourth, to reduce false positives and negatives,
\racepro uses \emph{replay and go-live} to validate detected races.  A
detected race based on the micro-operations may be either \emph{benign} or
\emph{harmful}, depending on whether it leads to a \emph{failure}, such as a
segmentation fault or a program abort.  \racepro considers a change in the order
of the system calls involved in a race to be an \emph{execution branch}.  To
check whether this branch leads to a failure, \racepro replays the recorded
execution until the \emph{reordered} system calls then resumes live execution.
It then runs a set of built-in or user-provided checkers on the live execution
to detect failures, and emits a bug report only when a real failure is detected.

This constitutes a departure from deterministic replay. First, \racepro
replays a modified version of the original execution that includes reordered system calls.
Second, after the reordered system calls are replayed, \racepro switches from a
controlled execution to a live execution. This can be challenging as the live
execution may use OS resources referenced during the controlled execution (e.g.
file descriptors), making the replay mechanism more complex than \scribe's.
Despite difficulties, our experimental results show that \racepro can detect
real bugs due to process races in widespread Linux deployed systems, including
several previously unknown bugs in shells, databases, and makefiles.
We found the practicality of \racepro limited as we obtained
best results only with user-provided race checkers as the built-in ones
were too rudimentary. An ideal build-in checker would attempt replaying the rest
of the original execution past the reordered system calls and measure how much
the replayed execution diverged from the original. Replying a modified execution
in the general case requires a much more evolved replayer, leading us to \dora.

Third, we present \dora (\S\ref{ch:dora}), a record-replay system which
allows a recorded execution of an application to be replayed with a modified
version of the application in the general case. We call this feature {\em mutable}
replay.  This feature, not available in previous
record-replay systems, enables powerful new functionality. In particular, \dora
can help reproduce, diagnose, and fix software bugs by replaying a version of a
recorded application that is recompiled with debugging information, reconfigured
to produce verbose log output, modified to include additional print statements,
or patched to fix a bug.
We introduce the concept of mutable replay. Adding a \code{printf()} call
to the replayed application is intuitively safe and the expected outcome clear,
but changing thousands of lines of code in the application may incur significant
differences from the original execution. Intuitively, a mutable replay system
must find a execution that corresponds as much as possible to the original
execution. We model the differences of two executions with a user-defined cost
function, and provide a generic one that works well in most cases.

\dora consists of three components: (1) a recorder that records application
execution to a log similar to the \scribe engine, (2) a replayer that can replay
a modified version of the application using the log, and (3) an explorer that
uses the replayer to find the execution of the modified program that best
corresponds to the log file. The recorder logs not only nondeterministic
interactions but also deterministic information such as system call arguments,
to allow the replayer to identify when a replay diverges from the original
execution early.
The explorer evaluates several possible execution paths to find a successful
mutable replay. It performs a best-first search for an execution of
the modified program that is as close to the original execution as
possible according to some cost function. It begins by replaying a
recorded execution on a modified program. When the replay diverges
from the original execution, the explorer tries to determine
why. For example, suppose the modified program made an unexpected
\code{printf()} call. This could be a new call to produce debugging
information, or it could simply occur earlier than expected because
code was deleted. The explorer chooses the most promising possibility
and communicates its decision to the replayer. This process repeats
until a successful execution is found.

{\dora} is designed to handle an wide range of real-world programs, including
multi-threaded applications. It can support a broad range of useful application
changes, but cannot support arbitrary changes; major changes to the process
layout or shared memory layout are not supported. Despite this limitation,
{\dora} is useful in a wide range of real-world use cases for testing,
debugging, and validating application changes. In fact, we even found a
previously unknown bug in Apache using {\dora}. {\dora}'s usefulness in practice
makes sense given that bug fixes tend to be relatively small and rarely change
core application semantics.

Lastly, we present \synapse (\S\ref{ch:synapse}), an heterogeneous database (DB)
replication system specifically designed for web applications. These web
applications behave very differently compared to traditional single-host
applications as they are distributed and have their state contained in
databases. Typically, web applications are comprised of many different
services, each implementing a specific feature, using a specific database.
For example, the recommendation feature of an e-commerce store can be
implemented in a separate service powered by a graph DB, while the store
frontend runs on a traditional SQL DB. These services share a common subset
of the data; for example, the recommendation feature would share the product
and user data with the store frontend.
Application state modifications consist of database primitive changes, such as a
node insertion in a graph DB, or a row update in a SQL DB. 
Designing a system that allows this common data subset to be synchronized across
all the different DBs is challenging for four reasons. First, it should be
compatible with a vast number of DBs, whose layouts and engines may be
completely different. Second, it should be easy to use: orchestrating the data
flows inside the internal eco-system of services should be seamless for developers.
Third, it should provide good consistency guarantees at scale, and fourth
the replication mechanism should be failure tolerant. Specifically, network partitions
should not result in having half of the DBs missing some data.

Using \dora to record the source DB transparently and replying its execution on
the destination DB would not work due to large differences between the two DB systems
as \dora would fail at matching a stream of system calls.
Instead, \synapse operates at the application level.
On a high level, \synapse records application state modifications at the source
application, and replay these modifications at the destination application,
making \synapse a mutable replay engine for web applications.
Typically, web applications are structured following
the model-view-controller (MVC) pattern. Data is accessed following an object-oriented
abstraction with {\em Models}, such as a \code{User} class with attributes such
as \code{email} and \code{name}. Models are implemented on top of
Object/Relational Mappers (ORMs).
The ORM does the heavy lifting of interacting with the DB so developers
do not have to write DB queries. Over the years, many ORMs have been developed,
each one targeting a different DB. Thankfully, all these ORMs expose a similar API
to developers to interact with the DB. For example, invoking
\code{User.create()} would create a new user, regardless of the combination
ORM/DB.  \synapse interposes on these ORMs to monitor accesses to data objects.
This allow \synapse to replicate data from
one DB to another, effectively replicating application state,
without developer intervention and with little DB-specific code.
Further, \synapse provides an easy-to-use API to describe data flows. Developers simply
annotate their models with \synapse's \code{publish} and \code{subscribe}
keywords to connect data models together.
Despite this simple API, developers can describe complex eco-systems
of services. \synapse provides causal consistency delivery semantics by
transparently intercepting and ordering all read and write queries to the DB in
a similar fashion to \scribe's rendezvous points. Finally \synapse provides
a fault-tolerant replication mechanism by implementing two-phase commits all
the way from the source DB to the destination DB.

We have implemented \synapse for Ruby-on-Rails. We present some experimental
data showing that \synapse scales well up to 60,000 updates/second for various
workloads.  We and others have built or modified 14 web applications to share
data with one another via \synapse. Those built by others have been deployed in
production by a startup, \crowdtap.  The applications we built extend popular
open-source applications to integrate them into data-driven ecosystems.
